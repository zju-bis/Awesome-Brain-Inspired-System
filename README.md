# Awesome-Brain-Inspired-System

This repository collected papers from influential conferences and journals (AI-oriented CCF-A from 2021 to 2024) related to brain-inspired systems. You can add more related papers by pulling the request. ğŸ˜Š

**Suggested follow-up research sequence for future contributors**: 
  - AI-related: AAAI->ICLR->IJCAI->MM->ICML->NIPS
  - Sys-related: TODO

## Contents

 - [Papers](#papers)
   - [Network Architecture Search](#network-architecture-search)
   - [Spiking Graph Neural Network](#Spiking-Graph-Neural-Network)
   - [Spiking Attention Mechanism](#Spiking-Attention-Mechanism)
   - [Spiking Recurrent Neural Network](#Spiking-Recurrent-Neural-Network)
   - [Spiking Convolutional Neural Network](#Spiking-Convolutional-Neural-Network)
   - [Spike-driven MLP](#Spike-driven-MLP)
   - [Lightweight Spiking Neural Networks](#Lightweight-Spiking-Neural-Networks)
   - [ANN2SNN](#ann2snn)
   - [Bio-inspired Neuron Optimization](#Bio-inspired-Neuron-Optimization)
   - [Bio-inspired Training Loss Optimization](#Bio-inspired-Training-Loss-Optimization)
   - [Surrogate Gradient Optimization](#Surrogate-Gradient-Optimization)
   - [Hybrid Learning](#Hybrid-Learning)
   - [Adversarial Spiking Neural Networks](#Adversarial-Spiking-Neural-Networks)
   - [Continual Learning with Spiking Neural Networks](#Continual-Learning-with-Spiking-Neural-Networks)
   - [Online Learning with Spiking Neural Networks](#Online-Learning-with-Spiking-Neural-Networks)
   - [Inference Acceleration](#Inference-Acceleration)



## TODO List

 - [] Missing AAAI 2025
 - [] Missing ICLR 2025
 - [] Missing IJCAI 2025
 - [] Missing MM 2025
 - [] Missing ICML 2025
 - [] Missing NIPS 2025

## Papers

### Network Architecture Search

 - AutoSNN: Towards Energy-Efficient Spiking Neural Networks
 - Emergence of Hierarchical Layers in a Single Sheet of Self-Organizing Spiking Neurons
 - DifferentiableÂ hierarchicalÂ andÂ surrogateÂ gradientÂ searchÂ forÂ spikingÂ neuralÂ networks
 - ESL-SNNs:Â AnÂ EvolutionaryÂ StructureÂ LearningÂ StrategyÂ forÂ SpikingÂ Neural Networks

### Spiking Graph Neural Network

  - A graph is worth 1-bit spikes: when graph contrastive learning meets spiking neural networks
  - ExploitingÂ SpikingÂ DynamicsÂ withÂ Spatial-temporalÂ FeatureÂ NormalizationÂ inÂ GraphÂ Learning
  - SpikingÂ GraphÂ ConvolutionalÂ Networks
  - DynamicÂ SpikingÂ GraphÂ NeuralÂ Networks
  - DynamicÂ ReactiveÂ SpikingÂ GraphÂ NeuralÂ Network
  - ScalingÂ UpÂ DynamicÂ GraphÂ RepresentationÂ LearningÂ viaÂ SpikingÂ NeuralÂ Networks
  - Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning

### Spiking Attention Mechanism

 - LMUFormer: Low complexity yet powerful spiking model with Legendre memory units
 - Spikformer: when spiking neural network meets transformer
 - Spike-driven Transformer
 - MaskedÂ SpikingÂ Transformer
 - SpikingÂ TransformersÂ forÂ Event-basedÂ SingleÂ ObjectÂ Tracking
 - Spatial-TemporalÂ Self-AttentionÂ forÂ AsynchronousÂ SpikingÂ NeuralÂ Networks
 - SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation
 - ComplexÂ DynamicÂ NeuronsÂ ImprovedÂ SpikingÂ TransformerÂ NetworkÂ forÂ EfficientÂ AutomaticÂ SpeechÂ Recognition
 - One-step Spiking Transformer with a Linear Complexity
 - TIM: An Efficient Temporal Interaction Module for Spiking Transformer
 - Efficient and Effective Time-Series Forecasting with Spiking Neural Networks (ICML'2024)
 - SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms
 - PSSD-Transformer: Powerful Sparse Spike-Driven Transformer for Image Semantic Segmentation
 - Towards High-performance Spiking Transformers from ANN to SNN Conversion
 - Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators (NIPS'2024)

### Spiking Recurrent Neural Network

 - Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks
 - SpikingÂ NeuralÂ NetworksÂ withÂ ImprovedÂ InherentÂ RecurrenceÂ DynamicsÂ forÂ SequentialÂ Learning
 - RSNN: Recurrent Spiking Neural Networks for Dynamic Spatial-Temporal Information Processing

### Spiking Convolutional Neural Network

 - spiking convolutional neural networks for text classification
 - Spiking PointNet: Spiking Neural Networks for Point Clouds
 - DeepÂ ResidualÂ LearningÂ inÂ SpikingÂ NeuralÂ Networks
 - DeepÂ Directly-TrainedÂ SpikingÂ NeuralÂ NetworksÂ forÂ ObjectÂ Detection
 - Temporal-wiseÂ AttentionÂ SpikingÂ NeuralÂ NetworksÂ forÂ EventÂ StreamsÂ Classification
 - Event-basedÂ ActionÂ RecognitionÂ UsingÂ MotionÂ InformationÂ andÂ SpikingÂ NeuralÂ Networks
 - Energy-EfficientÂ ModelsÂ forÂ High-DimensionalÂ SpikeÂ TrainÂ ClassificationÂ usingÂ SparseÂ SpikingÂ NeuralÂ Networks
 - Point-to-SpikeÂ ResidualÂ LearningÂ forÂ Energy-Efï¬cientÂ 3DÂ PointÂ CloudÂ Classiï¬cation
 - GatedÂ AttentionÂ CodingÂ forÂ TrainingÂ High-PerformanceÂ andÂ EfficientÂ SpikingÂ NeuralÂ Networks
 - Temporal-CodedÂ DeepÂ SpikingÂ NeuralÂ NetworkÂ withÂ EasyÂ TrainingÂ andÂ RobustÂ Performance
 - Learning A Spiking Neural Network for Efficient Image Deraining

### Spike-driven MLP

 - LearningÂ toÂ Time-DecodeÂ inÂ SpikingÂ NeuralÂ NetworksÂ ThroughÂ theÂ InformationÂ Bottleneck
 - RecognizingÂ High-SpeedÂ MovingÂ ObjectsÂ withÂ SpikeÂ Camera
 - Event-EnhancedÂ Multi-ModalÂ SpikingÂ NeuralÂ NetworkÂ forÂ DynamicÂ ObstacleÂ Avoidance
 - Brain-inspiredÂ MultilayerÂ PerceptronÂ withÂ SpikingÂ Neurons
 - Event-basedÂ VideoÂ ReconstructionÂ viaÂ Potential-assistedÂ SpikingÂ NeuralÂ Network
 - AÂ LowÂ LatencyÂ AdaptiveÂ CodingÂ SpikeÂ FrameworkÂ forÂ DeepÂ ReinforcementÂ Learning
 - SpikingÂ NeRF:Â RepresentingÂ theÂ Real-WorldÂ GeometryÂ byÂ aÂ DiscontinuousÂ Representation
 - spikepoint: an efficient point-based spiking neural network for event cameras'action recognition
 - Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera
 - Learning Optical Flow from Continuous Spike Streams
 - Multi-SacleÂ DynamicÂ CodingÂ ImprovedÂ SpikingÂ ActorÂ NetworkÂ forÂ ReinforcementÂ Learning
 - FullyÂ SpikingÂ VariationalÂ Autoencoder

### Lightweight Spiking Neural Networks

 - State Transition of Dendritic Spines Improves Learning of Sparse Spiking Neural Networks
 - towards energy efficient spiking neural networks: an unstructured pruning framework
 - sparse spiking neural network: exploring heterogeneity in time scales for pruning recurrent SNN
 - LitE-SNN: Designing Lightweight and Efficient Spiking Neural Network through Spatial-Temporal Compressive Network Search and Joint Optimization
 - Towards Efficient Deep Spiking Neural Networks Construction with Spiking Activity based Pruning
 - Reversing Structural Pattern Learning with Biologically Inspired Knowledge Distillation for Spiking Neural Networks
 - Q-SNNs: Quantized Spiking Neural Networks

### ANN2SNN

 - A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration
 - bridging the gap between ANNs and SNNs by calibrating offset spikes
 - optimal ANN-SNN conversion for high accuracy and ultra-low-latency spiking neural networks
 - optimal conversion of conventional artificial neural networks to spiking neural networks
 - EfficientÂ ConvertedÂ SpikingÂ NeuralÂ NetworkÂ forÂ 3DÂ andÂ 2DÂ Classification
 - ConstructingÂ DeepÂ SpikingÂ NeuralÂ NetworksÂ fromÂ ArtificialÂ NeuralÂ NetworksÂ withÂ KnowledgeÂ Distillation
 - OptimalÂ ANN-SNNÂ ConversionÂ forÂ FastÂ andÂ AccurateÂ InferenceÂ inÂ DeepÂ SpikingÂ NeuralÂ Networks
 - EfficientÂ andÂ AccurateÂ ConversionÂ ofÂ SpikingÂ NeuralÂ NetworkÂ withÂ BurstÂ Spikes
 - SpikeConverter:Â AnÂ EfficientÂ ConversionÂ FrameworkÂ ZippingÂ theÂ GapÂ betweenÂ ArtificialÂ NeuralÂ NetworksÂ andÂ SpikingÂ NeuralÂ Networks
 - OptimizedÂ PotentialÂ InitializationÂ forÂ Low-LatencyÂ SpikingÂ NeuralÂ Networks
 - NearÂ LosslessÂ TransferÂ LearningÂ forÂ SpikingÂ NeuralÂ Networks
 - StrategyÂ andÂ BenchmarkÂ forÂ ConvertingÂ DeepÂ Q-NetworksÂ toÂ Event-DrivenÂ SpikingÂ NeuralÂ Networks
 - Apprenticeship-Inspired Elegance: Synergistic Knowledge Distillation Empowers Spiking Neural Networks for Efficient Single-Eye Emotion Recognition
 - SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN

### Bio-inspired Neuron Optimization

 - Learning delays in spiking neural networks using dilated convolutions with learnable spacings
 - A progressive training framework for spiking neural networks with a learnable multi-hierarchical model
 - Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons
 - Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies
 - GLIF:Â AÂ Uniï¬edÂ GatedÂ LeakyÂ Integrate-and-FireÂ NeuronÂ forÂ SpikingÂ NeuralÂ Networks
 - LTMD:Â LearningÂ ImprovementÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ LearnableÂ ThresholdingÂ NeuronsÂ andÂ ModerateÂ Dropout
 - BiologicallyÂ InspiredÂ DynamicÂ ThresholdsÂ forÂ SpikingÂ NeuralÂ Networks
 - Temporal-CodedÂ SpikingÂ NeuralÂ NetworksÂ withÂ DynamicÂ FiringÂ Threshold:Â LearningÂ withÂ Event-DrivenÂ Backpropagation
 - SSF:Â AcceleratingÂ TrainingÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ StabilizedÂ SpikingÂ Flow
 - IncorporatingÂ LearnableÂ MembraneÂ TimeÂ ConstantÂ toÂ EnhanceÂ LearningÂ ofÂ SpikingÂ NeuralÂ Networks
 - TernaryÂ Spike:Â LearningÂ TernaryÂ SpikesÂ forÂ SpikingÂ NeuralÂ Networks
 - TC-LIF:Â AÂ Two-CompartmentÂ SpikingÂ NeuronÂ ModelÂ forÂ Long-TermÂ SequentialÂ Modelling
 - DeepÂ SpikingÂ NeuralÂ NetworkÂ withÂ NeuralÂ OscillationÂ andÂ Spike-PhaseÂ Information
 - High-Performance Temporal Reversible Spiking Neural Networks with $\mathcal{O}$(L) Training Memory and $\mathcal{O}$(1) Inference Cost
 - CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks
 - Autaptic Synaptic Circuit Enhances Spatio-temporal Predictive Learning of Spiking Neural Networks

### Bio-inspired Training Loss Optimization

 - IM-Loss:Â InformationÂ MaximizationÂ LossÂ forÂ SpikingÂ NeuralÂ Networks
 - Exploring Loss Functions for Time-based Training Strategy in Spiking Neural Networks
 - TrainingÂ SpikingÂ NeuralÂ NetworksÂ withÂ Event-drivenÂ Backpropagation
 - Backpropagated Neighborhood Aggregation for Accurate Training of Spiking Neural Networks
 - RMP-Loss:Â RegularizingÂ MembraneÂ PotentialÂ DistributionÂ forÂ SpikingÂ NeuralÂ Networks
 - Temporal-CodedÂ SpikingÂ NeuralÂ NetworksÂ withÂ DynamicÂ FiringÂ Threshold:Â LearningÂ withÂ Event-DrivenÂ Backpropagation
 - RecDis-SNN:Â RectifyingÂ MembraneÂ PotentialÂ DistributionÂ forÂ DirectlyÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - SpikeÂ CountÂ MaximizationÂ forÂ NeuromorphicÂ VisionÂ Recognition
 - EnhancingÂ TrainingÂ ofÂ SpikingÂ NeuralÂ NetworkÂ withÂ StochasticÂ Latency
 - EnhancingÂ RepresentationÂ ofÂ SpikingÂ NeuralÂ NetworksÂ viaÂ Similarity-SensitiveÂ ContrastiveÂ Learning
 - AnÂ EfficientÂ KnowledgeÂ TransferÂ StrategyÂ forÂ SpikingÂ NeuralÂ NetworksÂ fromÂ StaticÂ toÂ EventÂ Domain

### Surrogate Gradient Optimization

 - Adaptive Smoothing Gradient Learning for Spiking Neural Networks
 - Surrogate Module Learning: Reduce the Gradient Error Accumulation in Training Spiking Neural Networks
 - Temporal efficient training of spiking neural network via gradient re-weighting
 - SparseÂ SpikingÂ GradientÂ Descent
 - DifferentiableÂ Spike:Â RethinkingÂ Gradient-DescentÂ forÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - OnlineÂ TrainingÂ ThroughÂ TimeÂ forÂ SpikingÂ NeuralÂ Networks
 - TowardsÂ Memory-Â andÂ Time-EfficientÂ BackpropagationÂ forÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - Multi-LevelÂ FiringÂ withÂ SpikingÂ DS-ResNet:Â EnablingÂ BetterÂ andÂ DeeperÂ Directly-TrainedÂ SpikingÂ NeuralÂ Networks
 - LearnableÂ SurrogateÂ GradientÂ forÂ DirectÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - TrainingÂ SpikingÂ NeuralÂ NetworksÂ withÂ AccumulatedÂ SpikingÂ Flow

### Hybrid Learning

 - adaptive deep spiking neural network with global-local learning via balanced excitation and inhibitory mechanism
 - sequence approximation using feedforward spiking neural network for spatiotemporal learning: theory and optimization methods
 - SEENN: Towards Temporal Spiking Early-Exit Neural Networks
 - EICIL: Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks
 - Towards Low-latency Event-based Visual Recognition with Hybrid Step-wise Distillation Spiking Neural Networks

### Adversarial Spiking Neural Networks

 - EnhancingÂ theÂ RobustnessÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ StochasticÂ GatingÂ Mechanisms
 - Certified adversarial robustness for rate-encoded spiking neural networks
 - Threaten Spiking Neural Networks Through Combining Rate and Temporal Information
 - SNN-RAT:Â Robustness-enhancedÂ SpikingÂ NeuralÂ NetworkÂ throughÂ RegularizedÂ AdversarialÂ Training
 - TowardÂ RobustÂ SpikingÂ NeuralÂ NetworkÂ AgainstÂ AdversarialÂ Perturbation
 - HIRE-SNN:Â HarnessingÂ theÂ InherentÂ RobustnessÂ ofÂ Energy-EfficientÂ DeepÂ SpikingÂ NeuralÂ NetworksÂ byÂ TrainingÂ withÂ CraftedÂ InputÂ Noise
 - Robust Stable Spiking Neural Networks
 - RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding

### Continual Learning with Spiking Neural Networks
 - A progressive training framework for spiking neural networks with a learnable multi-hierarchical model
 - Hebbian learning-based orthogonal projection for continual learning of spiking neural networks
 - TrainingÂ SpikingÂ NeuralÂ NetworksÂ withÂ LocalÂ TandemÂ Learning
 - EnhancingÂ EfficientÂ ContinualÂ LearningÂ withÂ DynamicÂ StructureÂ DevelopmentÂ ofÂ SpikingÂ NeuralÂ Networks
 - EfficientÂ SpikingÂ NeuralÂ NetworksÂ withÂ SparseÂ SelectiveÂ ActivationÂ forÂ ContinualÂ Learning

### Online Learning with Spiking Neural Networks

 - Online stabilization of spiking neural networks
 - NDOT: Neuronal Dynamics-based Online Training for Spiking Neural Networks

### Inference Acceleration
 - EC-SNN: Splitting Deep Spiking Neural Networks for Edge Devices
 - UnleashingÂ theÂ PotentialÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ DynamicÂ Confidence
 - DCT-SNN:Â UsingÂ DCTÂ toÂ DistributeÂ SpatialÂ InformationÂ overÂ TimeÂ forÂ Low-LatencyÂ SpikingÂ NeuralÂ Networks
 - ShrinkingÂ YourÂ TimeStep:Â TowardsÂ Low-LatencyÂ NeuromorphicÂ ObjectÂ RecognitionÂ withÂ SpikingÂ NeuralÂ Networks
 - Towards Efficient Spiking Transformer: a Token Sparsification Framework for Training and Inference Acceleration

