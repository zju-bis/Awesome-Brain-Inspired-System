# Awesome-Brain-Inspired-System

This repository collected papers from influential conferences and journals (AI-oriented CCF-A from 2021 to 2024) related to brain-inspired systems. You can add more related papers by pulling the request. ğŸ˜Š

**Suggested follow-up research sequence for future contributors**: 
  - AI-related: AAAI,ICLR,CVPR,ICCV,IJCAI,MM,ICML,NIPS
  - Sys-related: TODO

## Contents

 - [Papers](#papers)
   - [Network Architecture Search](#network-architecture-search)
   - [Spiking Graph Neural Network](#Spiking-Graph-Neural-Network)
   - [Spiking Attention Mechanism](#Spiking-Attention-Mechanism)
   - [Spiking Recurrent Neural Network](#Spiking-Recurrent-Neural-Network)
   - [Spiking Convolutional Neural Network](#Spiking-Convolutional-Neural-Network)
   - [Spike-driven MLP](#Spike-driven-MLP)
   - [Lightweight Spiking Neural Networks](#Lightweight-Spiking-Neural-Networks)
   - [ANN2SNN](#ann2snn)
   - [Bio-inspired Neuron Optimization](#Bio-inspired-Neuron-Optimization)
   - [Bio-inspired Training Loss Optimization](#Bio-inspired-Training-Loss-Optimization)
   - [Surrogate Gradient Optimization](#Surrogate-Gradient-Optimization)
   - [Hybrid Learning](#Hybrid-Learning)
   - [Adversarial Spiking Neural Networks](#Adversarial-Spiking-Neural-Networks)
   - [Continual Learning with Spiking Neural Networks](#Continual-Learning-with-Spiking-Neural-Networks)
   - [Online Learning with Spiking Neural Networks](#Online-Learning-with-Spiking-Neural-Networks)
   - [Inference Acceleration](#Inference-Acceleration)
   - [Solver with Spiking Neural Networks](#Solver-with-Spiking-Neural-Networks)


## TODO List

 - [] 2025å¹´æš‚ç¼º

## Papers

### Network Architecture Search

 - AutoSNN: Towards Energy-Efficient Spiking Neural Networks (ICML'2022)
 - Emergence of Hierarchical Layers in a Single Sheet of Self-Organizing Spiking Neurons (NIPS'2022)
 - DifferentiableÂ hierarchicalÂ andÂ surrogateÂ gradientÂ searchÂ forÂ spikingÂ neuralÂ networks (NIPS'2022)
 - ESL-SNNs:Â AnÂ EvolutionaryÂ StructureÂ LearningÂ StrategyÂ forÂ SpikingÂ Neural Networks (AAAI'2023)

### Spiking Graph Neural Network

  
  - ExploitingÂ SpikingÂ DynamicsÂ withÂ Spatial-temporalÂ FeatureÂ NormalizationÂ inÂ GraphÂ Learning (IJCAI'2021)
  - SpikingÂ GraphÂ ConvolutionalÂ Networks (IJCAI'2022)
  - ScalingÂ UpÂ DynamicÂ GraphÂ RepresentationÂ LearningÂ viaÂ SpikingÂ NeuralÂ Networks (AAAI'2023)
  - A graph is worth 1-bit spikes: when graph contrastive learning meets spiking neural networks (ICLR'2024)
  - DynamicÂ SpikingÂ GraphÂ NeuralÂ Networks (AAAI'2024)
  - DynamicÂ ReactiveÂ SpikingÂ GraphÂ NeuralÂ Network (AAAI'2024)
  - Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning (ICML'2024)
  - Spiking Graph Neural Network on Riemannian Manifolds (NIPS'2024)

### Spiking Attention Mechanism

 - SpikingÂ TransformersÂ forÂ Event-basedÂ SingleÂ ObjectÂ Tracking (CVPR'2022)
 - Spikformer: when spiking neural network meets transformer (ICLR'2023)
 - MaskedÂ SpikingÂ Transformer (CVPR'2023)
 - Spatial-TemporalÂ Self-AttentionÂ forÂ AsynchronousÂ SpikingÂ NeuralÂ Networks (IJCAI'2023)
 - ComplexÂ DynamicÂ NeuronsÂ ImprovedÂ SpikingÂ TransformerÂ NetworkÂ forÂ EfficientÂ AutomaticÂ SpeechÂ Recognition (AAAI'2023)
 - LMUFormer: Low complexity yet powerful spiking model with Legendre memory units (ICLR'2024)
 - Spike-driven Transformer (NIPS'2024)
 - SpikingBERT: Distilling BERT to Train Spiking Language Models Using Implicit Differentiation (AAAI'2024)
 - One-step Spiking Transformer with a Linear Complexity (IJCAI'2024)
 - TIM: An Efficient Temporal Interaction Module for Spiking Transformer (IJCAI'2024)
 - Efficient and Effective Time-Series Forecasting with Spiking Neural Networks (ICML'2024)
 - SpikeLM: Towards General Spike-Driven Language Modeling via Elastic Bi-Spiking Mechanisms (ICML'2024)
 - PSSD-Transformer: Powerful Sparse Spike-Driven Transformer for Image Semantic Segmentation (MM'2024)
 - Advancing Spiking Neural Networks for Sequential Modeling with Central Pattern Generators (NIPS'2024)
 - QKFormer: Hierarchical Spiking Transformer using Q-K Attention (NIPS'2024)
 - SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network (NIPS'2024)
 - Spike-based Neuromorphic Model for Sound Source Localization (NIPS'2024)
 - Spiking Token Mixer: An Event-Driven Friendly Former Structure for Spiking Neural Networks (NIPS'2024)
 - Spiking Transformer with Experts Mixture (NIPS'2024)

### Spiking Recurrent Neural Network
 
 - SpikingÂ NeuralÂ NetworksÂ withÂ ImprovedÂ InherentÂ RecurrenceÂ DynamicsÂ forÂ SequentialÂ Learning (AAAI'2022)
 - Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks (NIPS'2024)
 - RSNN: Recurrent Spiking Neural Networks for Dynamic Spatial-Temporal Information Processing (MM'2024)

### Spiking Convolutional Neural Network

 - Temporal-CodedÂ DeepÂ SpikingÂ NeuralÂ NetworkÂ withÂ EasyÂ TrainingÂ andÂ RobustÂ Performance (AAAI'2021)
 - Temporal-wiseÂ AttentionÂ SpikingÂ NeuralÂ NetworksÂ forÂ EventÂ StreamsÂ Classification (ICCV'2021)
 - Event-basedÂ ActionÂ RecognitionÂ UsingÂ MotionÂ InformationÂ andÂ SpikingÂ NeuralÂ Networks (IJCAI'2021)
 - DeepÂ ResidualÂ LearningÂ inÂ SpikingÂ NeuralÂ Networks (NIPS'2021)
 - Energy-EfficientÂ ModelsÂ forÂ High-DimensionalÂ SpikeÂ TrainÂ ClassificationÂ usingÂ SparseÂ SpikingÂ NeuralÂ Networks (KDD'2021)
 - DeepÂ Directly-TrainedÂ SpikingÂ NeuralÂ NetworksÂ forÂ ObjectÂ Detection (CVPR'2023)
 - spiking convolutional neural networks for text classification (ICLR'2023)
 - Point-to-SpikeÂ ResidualÂ LearningÂ forÂ Energy-Efï¬cientÂ 3DÂ PointÂ CloudÂ Classiï¬cation (AAAI'2024)
 - GatedÂ AttentionÂ CodingÂ forÂ TrainingÂ High-PerformanceÂ andÂ EfficientÂ SpikingÂ NeuralÂ Networks (AAAI'2024)
 - Learning A Spiking Neural Network for Efficient Image Deraining (IJCAI'2024)
 - Spiking PointNet: Spiking Neural Networks for Point Clouds (NIPS'2024)
 - Autonomous Driving with Spiking Neural Networks (NIPS'2024)

### Spike-driven MLP

 - LearningÂ toÂ Time-DecodeÂ inÂ SpikingÂ NeuralÂ NetworksÂ ThroughÂ theÂ InformationÂ Bottleneck (NIPS'2021)
 - Brain-inspiredÂ MultilayerÂ PerceptronÂ withÂ SpikingÂ Neurons (CVPR'2022)
 - Event-basedÂ VideoÂ ReconstructionÂ viaÂ Potential-assistedÂ SpikingÂ NeuralÂ Network (CVPR'2022)
 - Learning Optical Flow from Continuous Spike Streams (NIPS'2022)
 - Multi-SacleÂ DynamicÂ CodingÂ ImprovedÂ SpikingÂ ActorÂ NetworkÂ forÂ ReinforcementÂ Learning (AAAI'2022)
 - FullyÂ SpikingÂ VariationalÂ Autoencoder (AAAI'2022)
 - RecognizingÂ High-SpeedÂ MovingÂ ObjectsÂ withÂ SpikeÂ Camera (MM'2023)
 - Event-EnhancedÂ Multi-ModalÂ SpikingÂ NeuralÂ NetworkÂ forÂ DynamicÂ ObstacleÂ Avoidance (MM'2023)
 - AÂ LowÂ LatencyÂ AdaptiveÂ CodingÂ SpikeÂ FrameworkÂ forÂ DeepÂ ReinforcementÂ Learning (IJCAI'2023)
 - SpikingÂ NeRF:Â RepresentingÂ theÂ Real-WorldÂ GeometryÂ byÂ aÂ DiscontinuousÂ Representation (AAAI'2024)
 - spikepoint: an efficient point-based spiking neural network for event cameras'action recognition (ICLR'2024)
 - Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera (NIPS'2024)

### Lightweight Spiking Neural Networks

 - State Transition of Dendritic Spines Improves Learning of Sparse Spiking Neural Networks
 - towards energy efficient spiking neural networks: an unstructured pruning framework
 - sparse spiking neural network: exploring heterogeneity in time scales for pruning recurrent SNN
 - LitE-SNN: Designing Lightweight and Efficient Spiking Neural Network through Spatial-Temporal Compressive Network Search and Joint Optimization
 - Towards Efficient Deep Spiking Neural Networks Construction with Spiking Activity based Pruning
 - Reversing Structural Pattern Learning with Biologically Inspired Knowledge Distillation for Spiking Neural Networks
 - Q-SNNs: Quantized Spiking Neural Networks

### ANN2SNN

 - A Free Lunch From ANN: Towards Efficient, Accurate Spiking Neural Networks Calibration
 - bridging the gap between ANNs and SNNs by calibrating offset spikes
 - optimal ANN-SNN conversion for high accuracy and ultra-low-latency spiking neural networks
 - optimal conversion of conventional artificial neural networks to spiking neural networks
 - EfficientÂ ConvertedÂ SpikingÂ NeuralÂ NetworkÂ forÂ 3DÂ andÂ 2DÂ Classification
 - ConstructingÂ DeepÂ SpikingÂ NeuralÂ NetworksÂ fromÂ ArtificialÂ NeuralÂ NetworksÂ withÂ KnowledgeÂ Distillation
 - OptimalÂ ANN-SNNÂ ConversionÂ forÂ FastÂ andÂ AccurateÂ InferenceÂ inÂ DeepÂ SpikingÂ NeuralÂ Networks
 - EfficientÂ andÂ AccurateÂ ConversionÂ ofÂ SpikingÂ NeuralÂ NetworkÂ withÂ BurstÂ Spikes
 - SpikeConverter:Â AnÂ EfficientÂ ConversionÂ FrameworkÂ ZippingÂ theÂ GapÂ betweenÂ ArtificialÂ NeuralÂ NetworksÂ andÂ SpikingÂ NeuralÂ Networks
 - OptimizedÂ PotentialÂ InitializationÂ forÂ Low-LatencyÂ SpikingÂ NeuralÂ Networks
 - NearÂ LosslessÂ TransferÂ LearningÂ forÂ SpikingÂ NeuralÂ Networks
 - StrategyÂ andÂ BenchmarkÂ forÂ ConvertingÂ DeepÂ Q-NetworksÂ toÂ Event-DrivenÂ SpikingÂ NeuralÂ Networks
 - Apprenticeship-Inspired Elegance: Synergistic Knowledge Distillation Empowers Spiking Neural Networks for Efficient Single-Eye Emotion Recognition
 - SpikeZIP-TF: Conversion is All You Need for Transformer-based SNN
 - Towards High-performance Spiking Transformers from ANN to SNN Conversion (MM'2024)
 - EnOF-SNN: Training Accurate Spiking Neural Networks via Enhancing the Output Feature (NIPS'2024)
 - SpikedAttention: Training-Free and Fully Spike-Driven Transformer-to-SNN Conversion with Winner-Oriented Spike Shift for Softmax Operation (NIPS'2024)

### Bio-inspired Neuron Optimization

 - Learning delays in spiking neural networks using dilated convolutions with learnable spacings
 - A progressive training framework for spiking neural networks with a learnable multi-hierarchical model
 - Addressing the speed-accuracy simulation trade-off for adaptive spiking neurons
 - Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies
 - GLIF:Â AÂ Uniï¬edÂ GatedÂ LeakyÂ Integrate-and-FireÂ NeuronÂ forÂ SpikingÂ NeuralÂ Networks
 - LTMD:Â LearningÂ ImprovementÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ LearnableÂ ThresholdingÂ NeuronsÂ andÂ ModerateÂ Dropout
 - BiologicallyÂ InspiredÂ DynamicÂ ThresholdsÂ forÂ SpikingÂ NeuralÂ Networks
 - Temporal-CodedÂ SpikingÂ NeuralÂ NetworksÂ withÂ DynamicÂ FiringÂ Threshold:Â LearningÂ withÂ Event-DrivenÂ Backpropagation
 - SSF:Â AcceleratingÂ TrainingÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ StabilizedÂ SpikingÂ Flow
 - IncorporatingÂ LearnableÂ MembraneÂ TimeÂ ConstantÂ toÂ EnhanceÂ LearningÂ ofÂ SpikingÂ NeuralÂ Networks
 - TernaryÂ Spike:Â LearningÂ TernaryÂ SpikesÂ forÂ SpikingÂ NeuralÂ Networks
 - TC-LIF:Â AÂ Two-CompartmentÂ SpikingÂ NeuronÂ ModelÂ forÂ Long-TermÂ SequentialÂ Modelling
 - DeepÂ SpikingÂ NeuralÂ NetworkÂ withÂ NeuralÂ OscillationÂ andÂ Spike-PhaseÂ Information
 - High-Performance Temporal Reversible Spiking Neural Networks with $\mathcal{O}$(L) Training Memory and $\mathcal{O}$(1) Inference Cost
 - CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural Networks
 - Autaptic Synaptic Circuit Enhances Spatio-temporal Predictive Learning of Spiking Neural Networks
 - Rethinking the Membrane Dynamics and Optimization Objectives of Spiking Neural Networks (NIPS'2024)

### Bio-inspired Training Loss Optimization

 - IM-Loss:Â InformationÂ MaximizationÂ LossÂ forÂ SpikingÂ NeuralÂ Networks
 - Exploring Loss Functions for Time-based Training Strategy in Spiking Neural Networks
 - TrainingÂ SpikingÂ NeuralÂ NetworksÂ withÂ Event-drivenÂ Backpropagation
 - Backpropagated Neighborhood Aggregation for Accurate Training of Spiking Neural Networks
 - RMP-Loss:Â RegularizingÂ MembraneÂ PotentialÂ DistributionÂ forÂ SpikingÂ NeuralÂ Networks
 - Temporal-CodedÂ SpikingÂ NeuralÂ NetworksÂ withÂ DynamicÂ FiringÂ Threshold:Â LearningÂ withÂ Event-DrivenÂ Backpropagation
 - RecDis-SNN:Â RectifyingÂ MembraneÂ PotentialÂ DistributionÂ forÂ DirectlyÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - SpikeÂ CountÂ MaximizationÂ forÂ NeuromorphicÂ VisionÂ Recognition
 - EnhancingÂ TrainingÂ ofÂ SpikingÂ NeuralÂ NetworkÂ withÂ StochasticÂ Latency
 - EnhancingÂ RepresentationÂ ofÂ SpikingÂ NeuralÂ NetworksÂ viaÂ Similarity-SensitiveÂ ContrastiveÂ Learning
 - AnÂ EfficientÂ KnowledgeÂ TransferÂ StrategyÂ forÂ SpikingÂ NeuralÂ NetworksÂ fromÂ StaticÂ toÂ EventÂ Domain
 - Exact Gradients for Stochastic Spiking Neural Networks Driven by Rough Signals (NIPS'2024)

### Surrogate Gradient Optimization

 - Adaptive Smoothing Gradient Learning for Spiking Neural Networks
 - Surrogate Module Learning: Reduce the Gradient Error Accumulation in Training Spiking Neural Networks
 - Temporal efficient training of spiking neural network via gradient re-weighting
 - SparseÂ SpikingÂ GradientÂ Descent
 - DifferentiableÂ Spike:Â RethinkingÂ Gradient-DescentÂ forÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - OnlineÂ TrainingÂ ThroughÂ TimeÂ forÂ SpikingÂ NeuralÂ Networks
 - TowardsÂ Memory-Â andÂ Time-EfficientÂ BackpropagationÂ forÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - Multi-LevelÂ FiringÂ withÂ SpikingÂ DS-ResNet:Â EnablingÂ BetterÂ andÂ DeeperÂ Directly-TrainedÂ SpikingÂ NeuralÂ Networks
 - LearnableÂ SurrogateÂ GradientÂ forÂ DirectÂ TrainingÂ SpikingÂ NeuralÂ Networks
 - TrainingÂ SpikingÂ NeuralÂ NetworksÂ withÂ AccumulatedÂ SpikingÂ Flow 
 - Advancing Training Efficiency of Deep Spiking Neural Networks through Rate-based Backpropagation (NIPS'2024)
 - Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks (NIPS'2024)

### Hybrid Learning

 - adaptive deep spiking neural network with global-local learning via balanced excitation and inhibitory mechanism
 - sequence approximation using feedforward spiking neural network for spatiotemporal learning: theory and optimization methods
 - SEENN: Towards Temporal Spiking Early-Exit Neural Networks
 - EICIL: Joint Excitatory Inhibitory Cycle Iteration Learning for Deep Spiking Neural Networks
 - Towards Low-latency Event-based Visual Recognition with Hybrid Step-wise Distillation Spiking Neural Networks
 - piking Neural Network as Adaptive Event Stream Slicer (NIPS'2024)

### Adversarial Spiking Neural Networks

 - EnhancingÂ theÂ RobustnessÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ StochasticÂ GatingÂ Mechanisms
 - Certified adversarial robustness for rate-encoded spiking neural networks
 - Threaten Spiking Neural Networks Through Combining Rate and Temporal Information
 - SNN-RAT:Â Robustness-enhancedÂ SpikingÂ NeuralÂ NetworkÂ throughÂ RegularizedÂ AdversarialÂ Training
 - TowardÂ RobustÂ SpikingÂ NeuralÂ NetworkÂ AgainstÂ AdversarialÂ Perturbation
 - HIRE-SNN:Â HarnessingÂ theÂ InherentÂ RobustnessÂ ofÂ Energy-EfficientÂ DeepÂ SpikingÂ NeuralÂ NetworksÂ byÂ TrainingÂ withÂ CraftedÂ InputÂ Noise
 - Robust Stable Spiking Neural Networks
 - RSC-SNN: Exploring the Trade-off Between Adversarial Robustness and Accuracy in Spiking Neural Networks via Randomized Smoothing Coding
 - FEEL-SNN: Robust Spiking Neural Networks with Frequency Encoding and Evolutionary Leak Factor (NIPS'2024)

### Continual Learning with Spiking Neural Networks
 - A progressive training framework for spiking neural networks with a learnable multi-hierarchical model
 - Hebbian learning-based orthogonal projection for continual learning of spiking neural networks
 - TrainingÂ SpikingÂ NeuralÂ NetworksÂ withÂ LocalÂ TandemÂ Learning
 - EnhancingÂ EfficientÂ ContinualÂ LearningÂ withÂ DynamicÂ StructureÂ DevelopmentÂ ofÂ SpikingÂ NeuralÂ Networks
 - EfficientÂ SpikingÂ NeuralÂ NetworksÂ withÂ SparseÂ SelectiveÂ ActivationÂ forÂ ContinualÂ Learning

### Online Learning with Spiking Neural Networks

 - Online stabilization of spiking neural networks
 - NDOT: Neuronal Dynamics-based Online Training for Spiking Neural Networks
 - Continuous Spatiotemporal Events Decoupling through Spike-based Bayesian Computation (NIPS'2024)

### Inference Acceleration
 - EC-SNN: Splitting Deep Spiking Neural Networks for Edge Devices
 - UnleashingÂ theÂ PotentialÂ ofÂ SpikingÂ NeuralÂ NetworksÂ withÂ DynamicÂ Confidence
 - DCT-SNN:Â UsingÂ DCTÂ toÂ DistributeÂ SpatialÂ InformationÂ overÂ TimeÂ forÂ Low-LatencyÂ SpikingÂ NeuralÂ Networks
 - ShrinkingÂ YourÂ TimeStep:Â TowardsÂ Low-LatencyÂ NeuromorphicÂ ObjectÂ RecognitionÂ withÂ SpikingÂ NeuralÂ Networks
 - Towards Efficient Spiking Transformer: a Token Sparsification Framework for Training and Inference Acceleration

### Solver with Spiking Neural Networks
  - Slack-Free Spiking Neural Network Formulation for Hypergraph Minimum Vertex Cover (NIPS'2024)
